<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on Hgt的博客</title>
    <link>https://hgt312.github.io/tags/deep-learning/</link>
    <description>Recent content in Deep Learning on Hgt的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 05 Dec 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://hgt312.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deep-learning笔记——神经网络的调试与优化（二）</title>
      <link>https://hgt312.github.io/post/dl_note3/</link>
      <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://hgt312.github.io/post/dl_note3/</guid>
      <description>神经网络的初始化 对几种初始化方法做个对比，发现其实初始化的选择还是很重要的。 全零初始化 全零初始化即将W和b中的元素全部设为0，这样会导致神经</description>
    </item>
    
    <item>
      <title>Deep-learning笔记——神经网络的调试与优化（一）</title>
      <link>https://hgt312.github.io/post/dl_note2/</link>
      <pubDate>Sun, 03 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://hgt312.github.io/post/dl_note2/</guid>
      <description>正则化 正则化是一种有效防止过拟合的手段。通过限制W的参数大小，从而减小模型的方差，以达到减少过拟合的效果。 常用的正则化方法有两种： L1正则化</description>
    </item>
    
    <item>
      <title>Deep learning笔记——张量压缩（一）</title>
      <link>https://hgt312.github.io/post/dl_note1/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://hgt312.github.io/post/dl_note1/</guid>
      <description>参考Github项目：https://github.com/timgaripov/TensorNet-TF 参考论文链接：http://arx</description>
    </item>
    
    <item>
      <title>tensorflow学习笔记（一）</title>
      <link>https://hgt312.github.io/post/tf_note1/</link>
      <pubDate>Tue, 22 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://hgt312.github.io/post/tf_note1/</guid>
      <description>该笔记使用斯坦福的tensorflow课程CS20SI作为教程，可以在https://web.stanford.edu/class/cs20</description>
    </item>
    
  </channel>
</rss>